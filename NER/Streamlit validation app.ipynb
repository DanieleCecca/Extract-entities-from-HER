{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Creating a Streamlit application to validate entities\n","\n","A Streamlit app has been created to validate the NER dataset generated by the LLM. You can accept or remove labeled phrases from the dataset. The labeled words will be highlighted.\n","\n","\n"],"metadata":{"id":"YwZsyQm60jXQ"}},{"cell_type":"markdown","source":["## Imports and environment setup"],"metadata":{"id":"tVoigne2P4A4"}},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"Ul9d4RmZ0fnC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1751967725847,"user_tz":-120,"elapsed":19364,"user":{"displayName":"Daniele Cecca","userId":"16870498098488741092"}},"outputId":"f5296d8e-5146-4566-cbf1-b9903db8c9d6"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/44.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for htbuilder (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip install streamlit st-annotated-text pyngrok -q"]},{"cell_type":"code","source":["import os\n","from pyngrok import conf\n","import pandas as pd"],"metadata":{"id":"uQeyFxG14slm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import userdata\n","# Salva la configurazione di ngrok\n","conf.get_default().auth_token = userdata.get(\"NGROK_AUTHTOKEN\")"],"metadata":{"id":"_DFf2X1XUSzM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3axwAdddkmqY","executionInfo":{"status":"ok","timestamp":1751967839475,"user_tz":-120,"elapsed":112554,"user":{"displayName":"Daniele Cecca","userId":"16870498098488741092"}},"outputId":"dea70343-3e0a-41a0-9878-3e255eb2d7a4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["## Constants"],"metadata":{"id":"KK3cA4n4P7Zd"}},{"cell_type":"code","source":["DATASET=\"/content/drive/MyDrive/SanRaffaele/Data/Dataset NER/NER_LLAMA70B.csv\""],"metadata":{"id":"ok1SoZSHT19K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Streamlit app"],"metadata":{"id":"HrXdS664Tl5M"}},{"cell_type":"code","source":["os.environ[\"STREAMLIT_DISABLE_WATCHDOG_WARNINGS\"] = \"true\"\n"],"metadata":{"id":"Nibw4kffcOan"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%writefile app.py\n","import streamlit as st\n","import pandas as pd\n","import ast\n","\n","st.set_page_config(page_title=\"NER Validator\", layout=\"wide\")\n","st.title(\"ðŸ“Œ NER Dataset Validator\")\n","\n","if \"data\" not in st.session_state:\n","  uploaded = st.file_uploader(\"Carica CSV\", type=\"csv\")\n","  if uploaded:\n","      df = pd.read_csv(uploaded)\n","      df[\"label\"] = df[\"label\"].apply(ast.literal_eval)\n","      st.session_state.data = df\n","      st.session_state.idx = 0\n","      st.session_state.results = []\n","      st.session_state.correct = 0\n","      st.session_state.wrong = 0\n","  else:\n","      st.stop()  # Fermiamo l'app finchÃ© non c'Ã¨ un file\n","\n","df = st.session_state.data\n","\n","total = len(df)\n","st.sidebar.metric(\"Totale righe\", total)\n","st.sidebar.metric(\"Corrette\", st.session_state.correct)\n","st.sidebar.metric(\"Sbagliate\", st.session_state.wrong)\n","st.sidebar.progress(st.session_state.idx / total if total > 0 else 0)\n","\n","def render_annotated(row):\n","  tokens = row[\"frase\"].split()\n","  labels = row[\"label\"]\n","  from annotated_text import annotated_text as show\n","  annotated = []\n","  ent = None\n","  for token, lab in zip(tokens, labels):\n","      if lab.startswith(\"B-\"):\n","          ent = lab[2:]\n","          annotated.append((token, ent))\n","      elif lab.startswith(\"I-\") and ent:\n","          annotated.append((token, ent))\n","      else:\n","          ent = None\n","          annotated.append(token+' ')\n","  show(*annotated)\n","\n","if st.session_state.idx < total:\n","  row = df.iloc[st.session_state.idx]\n","  st.subheader(f\"Riga {st.session_state.idx+1} / {total}\")\n","  render_annotated(row)\n","\n","  col1, col2 = st.columns(2)\n","  if col1.button(\"âœ… Corretta\"):\n","      st.session_state.results.append({**row, \"correct\": True})\n","      st.session_state.correct += 1\n","      st.session_state.idx += 1\n","      st.rerun()\n","  if col2.button(\"âŒ Sbagliata\"):\n","      st.session_state.results.append({**row, \"correct\": False})\n","      st.session_state.wrong += 1\n","      st.session_state.idx += 1\n","      st.rerun()\n","else:\n","  st.success(\"âœ… Validazione completata!\")\n","  df_res = pd.DataFrame(st.session_state.results)\n","  tp = len(df_res[df_res[\"correct\"]==True])\n","  tn = len(df_res[df_res[\"correct\"]==False])\n","  fp = 0\n","  fn = 0\n","\n","  acc = (tp + tn) / total\n","  prec = tp/(tp+fp) if tp+fp>0 else 0\n","  rec = tp/(tp+fn) if tp+fn>0 else 0\n","  f1 = 2*prec*rec/(prec+rec) if prec+rec>0 else 0\n","\n","  col1, col2, col3, col4 = st.columns(4)\n","  col1.metric(\"Accuracy\", f\"{acc:.2f}\")\n","  col2.metric(\"Precision\", f\"{prec:.2f}\")\n","  col3.metric(\"Recall\", f\"{rec:.2f}\")\n","  col4.metric(\"F1 score\", f\"{f1:.2f}\")\n","\n","  st.download_button(\"Scarica validato\", data=df_res.to_csv(index=False), file_name=\"validated.csv\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BJHSsjKR29Vt","executionInfo":{"status":"ok","timestamp":1751967839487,"user_tz":-120,"elapsed":16,"user":{"displayName":"Daniele Cecca","userId":"16870498098488741092"}},"outputId":"cd187dae-ad5f-4327-f693-03375b85d97f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing app.py\n"]}]},{"cell_type":"markdown","source":["Each user has a user_id obtained from the URL or generated as a UUID.\n","\n","The state.json file stores row assignments and validation results for all users.\n","\n","Each user sees a unique row that has not yet been validated or assigned.\n","\n","Once validated, the row is no longer shown to anyone.\n","\n","The results are displayed only when all rows have been validated."],"metadata":{"id":"AZQQ7ZlGfP4z"}},{"cell_type":"code","source":["  %%writefile app.py\n","\n","  import streamlit as st\n","  import pandas as pd\n","  import ast\n","  import json\n","  import os\n","  import uuid\n","  from annotated_text import annotated_text as show\n","\n","  DATA_FILE = \"/content/drive/MyDrive/SanRaffaele/Data/Dataset NER/NER_LLAMA70B.csv\"\n","  STATE_FILE = \"/content/drive/MyDrive/SanRaffaele/Data/Dataset NER/state.json\"\n","\n","\n","  def load_state():\n","      if os.path.exists(STATE_FILE):\n","          with open(STATE_FILE, \"r\") as f:\n","              return json.load(f)\n","      else:\n","          return {\"assigned\": {}, \"results\": {}}\n","\n","  def save_state(state):\n","      with open(STATE_FILE, \"w\") as f:\n","          json.dump(state, f)\n","\n","  def get_next_row(df, state, user_id):\n","      # Se l'utente ha giÃ  una riga assegnata non validata, torna quella\n","      if user_id in state[\"assigned\"]:\n","          idx = state[\"assigned\"][user_id]\n","          if str(idx) not in state[\"results\"]:\n","              return idx\n","          else:\n","              del state[\"assigned\"][user_id]\n","\n","      assigned_indices = set(state[\"assigned\"].values())\n","      validated_indices = set(int(i) for i in state[\"results\"].keys())\n","\n","      for i in range(len(df)):\n","          if i not in assigned_indices and i not in validated_indices:\n","              state[\"assigned\"][user_id] = i\n","              save_state(state)\n","              return i\n","      return None\n","\n","  def render_annotated(row):\n","      tokens = row[\"frase\"].split()\n","      labels = row[\"label\"]\n","      annotated = []\n","      ent = None\n","      for token, lab in zip(tokens, labels):\n","          if lab.startswith(\"B-\"):\n","              ent = lab[2:]\n","              annotated.append((token, ent))\n","          elif lab.startswith(\"I-\") and ent:\n","              annotated.append((token, ent))\n","          else:\n","              ent = None\n","              annotated.append(token+' ')\n","      show(*annotated)\n","\n","  # --- Streamlit app setup ---\n","\n","  st.set_page_config(page_title=\"NER Validator \", layout=\"wide\")\n","  st.title(\"ðŸ“Œ NER Dataset Validator MULTI \")\n","\n","  # Inizializza session_id se non esiste\n","  if \"session_id\" not in st.session_state:\n","      st.session_state.session_id = str(uuid.uuid4())\n","\n","  # Ottieni user_id da query params o genera fallback\n","  params = st.query_params\n","  user_id = params.get(\"user\", [f\"user_{st.session_state.session_id}\"])[0]\n","  st.session_state.user_id = user_id\n","\n","  st.write(f\"ðŸ‘¤ User ID: {user_id}\")\n","\n","  # Caricamento dati\n","  if \"data\" not in st.session_state:\n","\n","        df = pd.read_csv(DATA_FILE)\n","        # Converti stringhe label in liste reali\n","        df[\"label\"] = df[\"label\"].apply(ast.literal_eval)\n","        st.session_state.data = df\n","\n","  df = st.session_state.data\n","\n","  # Carica stato globale\n","  state = load_state()\n","\n","  idx = get_next_row(df, state, user_id)\n","\n","  if idx is None:\n","      st.success(\"âœ… Tutte le righe sono state validate!\")\n","\n","      results = state[\"results\"]\n","      if results:\n","          rows = []\n","          for k,v in results.items():\n","              row = df.iloc[int(k)].to_dict()\n","              row[\"correct\"] = v[\"correct\"]\n","              rows.append(row)\n","          df_res = pd.DataFrame(rows)\n","\n","          tp = len(df_res[df_res[\"correct\"]==True])\n","          tn = len(df_res[df_res[\"correct\"]==False])\n","          fp = 0\n","          fn = 0\n","\n","          total = len(df)\n","          acc = (tp + tn) / total\n","          prec = tp/(tp+fp) if tp+fp>0 else 0\n","          rec = tp/(tp+fn) if tp+fn>0 else 0\n","          f1 = 2*prec*rec/(prec+rec) if prec+rec>0 else 0\n","\n","          col1, col2, col3, col4 = st.columns(4)\n","          col1.metric(\"Accuracy\", f\"{acc:.2f}\")\n","          col2.metric(\"Precision\", f\"{prec:.2f}\")\n","          col3.metric(\"Recall\", f\"{rec:.2f}\")\n","          col4.metric(\"F1 score\", f\"{f1:.2f}\")\n","\n","          st.download_button(\"Scarica validato\", data=df_res.to_csv(index=False), file_name=\"validated.csv\")\n","  else:\n","      row = df.iloc[idx]\n","      st.subheader(f\"Riga {idx+1} / {len(df)}\")\n","\n","      render_annotated(row)\n","\n","  col1, col2 = st.columns(2)\n","  if col1.button(\"âœ… Corretta\"):\n","      state[\"results\"][str(idx)] = {\"correct\": True}\n","      if user_id in state[\"assigned\"]:\n","          del state[\"assigned\"][user_id]\n","      save_state(state)\n","      st.rerun()\n","\n","  if col2.button(\"âŒ Sbagliata\"):\n","      state[\"results\"][str(idx)] = {\"correct\": False}\n","      if user_id in state[\"assigned\"]:\n","          del state[\"assigned\"][user_id]\n","      save_state(state)\n","      st.rerun()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bpR18skclTE7","executionInfo":{"status":"ok","timestamp":1751967839498,"user_tz":-120,"elapsed":11,"user":{"displayName":"Daniele Cecca","userId":"16870498098488741092"}},"outputId":"dbefde94-5f10-4a78-9848-eeee3023228b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting app.py\n"]}]},{"cell_type":"code","source":["from pyngrok import ngrok\n","\n","# Kill eventuali vecchi tunnel\n","ngrok.kill()\n","\n","# Crea il tunnel\n","public_url = ngrok.connect(8502)\n","print(\"ðŸŒ URL pubblico:\", public_url)\n","# Disattiva file watcher (evita crash su Colab)\n","\n","# Avvia Streamlit\n","!STREAMLIT_SERVER_FILE_WATCHER_TYPE=none streamlit run app.py --server.port=8502 &> /content/logs.txt &\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q4KTBSCn3fpd","executionInfo":{"status":"ok","timestamp":1751967841850,"user_tz":-120,"elapsed":2349,"user":{"displayName":"Daniele Cecca","userId":"16870498098488741092"}},"outputId":"12643b7f-303c-4e2d-e97d-1856655126c1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ðŸŒ URL pubblico: NgrokTunnel: \"https://fead04bc2db8.ngrok-free.app\" -> \"http://localhost:8502\"\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"ke-7Og_JaOah"},"execution_count":null,"outputs":[]}]}